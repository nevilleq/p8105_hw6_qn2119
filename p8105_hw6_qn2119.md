P8105 Homework 6
================
Quinton Neville
November 20, 2018

Problem 1
=========

This problem concerns previously investigated data concerning the rate of homocide resolution in police departements in major cities around the country. Here, we will briefly clean and tidy these data and fit GLM logit models by city on unsolved with victim age, sex, and race as predictors, focusing on the coefficient for race, obtain 95% CI's for these coefficient estimates, and visualize the results.

#### Read and Clean Washington Post Data

``` r
#Read in the wp homicide data
wp.homicide.df <- read_csv("./data/wp_homicide_data.csv")

#Snag Dimensions and summary
dim.wp.df <- dim(wp.homicide.df)

#Unique City/State locations
unique.locations <- wp.homicide.df %>% distinct(., city, state) %>% nrow() 

#New city_state variable, filtering, and mutating
wp.homicide.df <- wp.homicide.df %>%
  mutate(
    city_state = str_c(city, state, sep = ", "), 
    unsolved = ifelse(disposition == "Closed without arrest" | disposition == "Open/No arrest", TRUE, FALSE)) %>% 
  select(city_state, everything()) %>%
  filter(city_state != "Dallas, TX" & city_state != "Phoenix, AZ" &
         city_state != "Kansas City, MO" & city_state != "Tulsa, AL") %>%
  mutate(
    victim_race = ifelse(victim_race == "White", "white", "non-white"), 
    victim_race = as.factor(victim_race) %>% fct_relevel(., "white", "non-white"),
    victim_age = parse_number(victim_age)
  )
```

#### Fit a Logit GLM for Baltimore, MD

Here, we fit a GLM from the binomial family, with the binary solved or unsolved variable as the response and victim age, sex, and race as predictors. We then obtained a 95% CI for the estimated coefficient for race, describing the change in log odds of the case being unsolved. Lastly, we produce a table of the exponentiated values, back transforming out of log-space to interpret the coefficients in terms of odds.

``` r
#GLM model Baltimore
baltimore.glm <- wp.homicide.df %>% 
  group_by(city_state) %>% 
  nest() %>% 
  filter(city_state == "Baltimore, MD") %>%
  mutate(glm_model = map(data, ~glm(unsolved ~ victim_age + victim_sex + victim_race, data = .x, family = "binomial")),
         conf_int = map(.x = glm_model, ~confint_tidy(.x, conf.level = 0.95)),
         glm_model = map(glm_model, ~broom::tidy(.x))) %>%
  unnest(., glm_model, conf_int) %>%
  filter(term == "victim_racenon-white") %>%
  select(city_state, estimate, conf.low, conf.high) %>%
  gather(key = type, value = value, estimate:conf.high) %>%
  mutate(value = exp(value))

baltimore.glm %>% knitr::kable()
```

| city\_state   | type      |     value|
|:--------------|:----------|---------:|
| Baltimore, MD | estimate  |  2.269591|
| Baltimore, MD | conf.low  |  1.613764|
| Baltimore, MD | conf.high |  3.203460|

The table obtained above describes that in Baltimore, MD, we would expect to see a 127% increase in the odds of a homocide being unsolved if the victim was not-white, after adjusting for victim age and sex. The corresponding 95% CI implies that we are 95% confident that the true odds ratio for non-white victim homocide resolution in Baltimore, MD lies between (1.61, 3.2). As this does not include 1, we have good evidence to suggest there is a significant increase in the odds of a homocide case being unsolved when the victim is non-white in Baltimore, MD, after adjusting for victim age and sex.

#### Repeat GLM Modeling for All Cities

We repeat the process outlined above for each city in these data. Again we fit the glm, obtain a 95% confidence interval for the coefficient estimate, exponetial transform to obtain the OR, and manipulate the resulting data frame for visualization.

``` r
#GLM all Cities
all.city.glm <- wp.homicide.df %>% 
  group_by(city_state) %>% 
  nest() %>%
  mutate(glm_model = map(data, ~glm(unsolved ~ victim_age + victim_sex + victim_race, data = .x, family = "binomial")),
         conf_int = map(.x = glm_model, ~confint_tidy(.x, conf.level = 0.95)),
         glm_model = map(glm_model, ~broom::tidy(.x))) %>%
  unnest(., glm_model, conf_int) %>%
  filter(term == "victim_racenon-white") %>%
  select(city_state, estimate, conf.low, conf.high) %>%
  mutate(city_state = as.factor(city_state),
         city_state = fct_reorder(city_state, estimate, .desc = FALSE)) %>%
  gather(key = type, value = value, estimate:conf.high) %>%
  mutate(value = exp(value)) %>%
  spread(type, value)
```

Here we visualize the resulting OR for a homocide being unsolved for non-white victims in each city, after adjusting for victim age and sex. Cities are found on the y-axis, OR are on the x-axis, and the red error bars describe the 95% confidence interval for the true OR.

``` r
all.city.glm %>%
  ggplot(aes(x = city_state, y = estimate, fill = estimate)) +
  geom_bar(stat = "identity", colour = "black", width = 1, alpha = 0.9) +
  geom_errorbar(aes(x = city_state, ymin = conf.low, ymax = conf.high), colour = "red", size = .9, alpha = 0.5) +
  coord_flip() +
  viridis::scale_fill_viridis(
    option = "magma",
    name = "Odds Ratio", 
    discrete = FALSE) +
  labs(
    x = "City",
    y = "Odds Ratio",
    title = "Odds of Unsolved Homocide for Non-White Victims") +
  theme(legend.position = "bottom",
        axis.text.y = element_text(color = "black", 
        size = 10,  hjust = 1)) +
  scale_y_continuous(breaks = seq(0, 24, 1), sec.axis = dup_axis())
```

<img src="p8105_hw6_qn2119_files/figure-markdown_github/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;" />

With respect to the visualization above, it is important to note that while all 95% CI's do not contain 1, almost every estimated coefficient is above 1. Additionally, 28 cities have lower 95% OR bounds above one, indicating that in over 50% of cities, we would expect to see a significant increase in the odds of a homicide being unsolved if the vitcim was non-white, adjusting for age and sex. We would conclude, based on these results and the visualization above, that there exists racial disparity in closing homicide cases in many major cities in the United States; adjusting for age and sex of the victim.

Problem 2
=========

These data concern a child's birthweight, along with a variety of other pre-existing and post-partum baby characteristics. Here we will clean and tidy the data as appropriate, propose an inital linear model, compare it with two alternate models using k-fold cross-validation, and make a conclusion based on the results.

``` r
child.bw.df <- read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
    babysex = ifelse(babysex == 1, "Male", "Female") %>% as.factor(),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
    malform = ifelse(malform == 1, TRUE, FALSE)
  ) %>%
  select(bwt, everything())

#Check NA
missing_vec <- apply(child.bw.df, 2, function(x) {sum(is.na(x))})
```

#### Modeling

Here, we propose a linear model for simplicity. We will utilize backwards subset selection based on AIC to iteratively eliminate the covariate which maximizes AIC, until a minimum AIC is achieved (i.e. if after removing a covariate, removing any other covariates increases the AIC, that covariate subset will be selected as optimal).

``` r
#Subset LM Selection (AIC)
mult.fit <- lm(bwt ~ ., data = child.bw.df)
back.sub.formula <- step(mult.fit, direction = 'backward')$terms
back.sub.mod <- lm(back.sub.formula, data = child.bw.df) 
```

After utilizing backwards subset selection based on AIC criterion, we propose the model

*B**i**r**t**h**w**e**i**g**h**t*<sub>*i*</sub> ∼ *H**e**a**d**C**i**r**c**u**m*.<sub>*i*</sub> + *L**e**n**g**t**h*<sub>*i*</sub> + *M**o**t**h**e**r*′*s**W**e**i**g**h**t*<sub>*i*</sub> + *F**a**m**i**l**y**I**n**c**o**m**e*<sub>*i*</sub> + *G**e**s**t*.*W**e**e**k**s*<sub>*i*</sub> + *M**o**t**h**e**r**H**e**i**g**h**t*<sub>*i*</sub> + *M**o**t**h**e**r**R**a**c**e*<sub>*i*</sub> + *P**a**r**i**t**y*<sub>*i*</sub> + *P**r**e*.*P**r**e**g*.*W**i**e**g**h**t*<sub>*i*</sub> + *S**m**o**k**e*<sub>*i*</sub> + *ε*<sub>*i*</sub>

Intuitively, many of these covariates would be expected to be linearly associated with a babie's birthweight. Interestingly, using the backwards subset selection method with AIC criterion did not select any covariates pertaining to the father, only the mother of the child. Below, we visualized the results of the regression output:

``` r
back.sub.mod %>% broom::tidy() %>%
  knitr::kable()
```

| term        |       estimate|    std.error|   statistic|    p.value|
|:------------|--------------:|------------:|-----------:|----------:|
| (Intercept) |  -6070.2638943|  136.9081478|  -44.338222|  0.0000000|
| babysexMale |    -28.5580171|    8.4548958|   -3.377690|  0.0007374|
| bhead       |    130.7770408|    3.4465672|   37.944144|  0.0000000|
| blength     |     74.9471109|    2.0190479|   37.120027|  0.0000000|
| delwt       |      4.1067316|    0.3920592|   10.474775|  0.0000000|
| fincome     |      0.3180229|    0.1747477|    1.819898|  0.0688436|
| gaweeks     |     11.5924873|    1.4620657|    7.928842|  0.0000000|
| mheight     |      6.5940377|    1.7848817|    3.694383|  0.0002231|
| mrace2      |   -138.7924801|    9.9070869|  -14.009414|  0.0000000|
| mrace3      |    -74.8867755|   42.3146313|   -1.769761|  0.0768374|
| mrace4      |   -100.6781427|   19.3246910|   -5.209819|  0.0000002|
| parity      |     96.3046933|   40.3362158|    2.387549|  0.0170038|
| ppwt        |     -2.6755853|    0.4273585|   -6.260752|  0.0000000|
| smoken      |     -4.8434197|    0.5855757|   -8.271210|  0.0000000|

Additionally, by selecting based on AIC criterion, it is interesting that every covariate other than financial income and mother being of asian descent is significant. However, even those covariates which are not significant, they are very nearly so and should not be discounted as unimportant simply based on p-value.

Next, we visualized the a residuals vs. fitted values plot to assess the linear model assumption of homoscedasticity as well as where our predictive model may be biased/highly variable.

``` r
child.bw.df %>%
  add_residuals(back.sub.mod) %>%
  add_predictions(back.sub.mod) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.8) +
  labs(
    x = "Fitted",
    y = "Residual",
    title = "Residuals vs. Fitted"
  )
```

<img src="p8105_hw6_qn2119_files/figure-markdown_github/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;" />

Observing the Residuals vs. Fitted values plot above, we noted that for those predicted birthweights above 2000 kg, the residuals are approximately equally distributed around zero with no particular trend and few outliers. However, for those fitted values less than 2000 kg, we see a non-linear trend in the residuals, implying unequal variance amongst covariates in the model. While this implies that assumptions are not met for inference, we may still be able to analyze this as a predictive model. Again we note that as a predictive model, it missess highly when predicting low birthweights but does a fairly good job for birthweights between 2000-4000 kg.

#### Comparing Models

Given our selected model above, we will fit two alternative linear models for birthweight

-   Baby length at birth and gestational age as predictors

-   Baby head circumference, length at birth, and sex; using all possible interactions.

We will compare our model with these two alternatives using 10-fold Cross validation with RMSE as the predictive quantity to minimize. Below we declare the models and visualize the summary output.

``` r
mod.2a <- child.bw.df %>% lm(bwt ~ blength + gaweeks, data = .)
mod.2a %>% broom::tidy() %>% knitr::kable()
```

| term        |     estimate|  std.error|  statistic|  p.value|
|:------------|------------:|----------:|----------:|--------:|
| (Intercept) |  -4347.66707|  97.958360|  -44.38281|        0|
| blength     |    128.55569|   1.989891|   64.60439|        0|
| gaweeks     |     27.04673|   1.717930|   15.74379|        0|

``` r
mod.2b <- child.bw.df %>% lm(bwt ~ bhead * babysex + bhead * blength + blength * babysex + bhead * babysex * blength, data = .)
mod.2b %>% broom::tidy() %>% knitr::kable()
```

| term                      |      estimate|     std.error|   statistic|    p.value|
|:--------------------------|-------------:|-------------:|-----------:|----------:|
| (Intercept)               |   -801.948671|  1102.3077046|  -0.7275180|  0.4669480|
| bhead                     |    -16.597546|    34.0916082|  -0.4868514|  0.6263883|
| babysexMale               |  -6374.868351|  1677.7669213|  -3.7996150|  0.0001469|
| blength                   |    -21.645964|    23.3720477|  -0.9261475|  0.3544209|
| bhead:babysexMale         |    198.393181|    51.0916850|   3.8830816|  0.0001047|
| bhead:blength             |      3.324444|     0.7125586|   4.6655020|  0.0000032|
| babysexMale:blength       |    123.772887|    35.1185360|   3.5244319|  0.0004288|
| bhead:babysexMale:blength |     -3.878053|     1.0566296|  -3.6702106|  0.0002453|

Next we utilze 10-Fold validation, with 100 iterations, to robustly assess any and all differences in RMSE that may exist between the data with varying training and test data sets.

``` r
cv.df <- crossv_mc(child.bw.df, 100) %>%
  mutate(train = map(train, as.tibble),
         test = map(test, as.tibble)) %>%
  mutate(back_sub_lm = map(train, ~lm(back.sub.formula, data = .x)),
         lm_2a = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         lm_2b = map(train, ~lm(bwt ~ bhead * babysex + bhead * blength + blength * babysex + bhead * babysex * blength, data = .x))) %>%
  mutate(back_sub_rmse = map2_dbl(back_sub_lm, test, ~rmse(model = .x, data = .y)),
         lm_2a_rmse = map2_dbl(lm_2a, test, ~rmse(model = .x, data = .y)),
         lm_2b_rmse = map2_dbl(lm_2b, test, ~rmse(model = .x, data = .y)))
```

Lastly, we visualize this output with a mixture violin/boxplot of RMSE by model below.

``` r
cv.df %>% 
  select(ends_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "_rmse", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin(trim = FALSE) + 
  geom_boxplot(width = 0.25) +
  labs(
    y = "Root Mean Sqaure Error",
    x = "Model",
    title = "RMSE by Model"
  ) 
```

<img src="p8105_hw6_qn2119_files/figure-markdown_github/unnamed-chunk-10-1.png" width="90%" style="display: block; margin: auto;" />

Here, we see that the model built with backward subset AIC criterion has the minimal RMSE of all three models, although the second linear model with interaction terms has only slightly higher RMSE with fewer terms. Lastly, the simplest linear model has the highest RMSE, which is not necessarily surprising. Overall, this leads us to conclude that the model built with backwards subset selection, is the best predictive model for minimizing RMSE, robustly confirmed with 100 iterative 10-fold cross validation. However, if inference were to be the goal of this model, we would suggest model assumptions and further analysis be conducted on the simpler linear models. Intuitively, we would expect the simplest linear model to have the strongest linear assumptions met, making it the best candidate of these three for inference.
